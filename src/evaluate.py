"""Model evaluation for GraphDiffLayout experiments."""

import numpy as np
import matplotlib.pyplot as plt
import cv2
from skimage.metrics import structural_similarity as ssim
from src.utils.graph_utils import compute_box_iou
from src.utils.visualization import save_figure
import os
import time

def evaluate_layout_text_alignment(layouts, img_nc_dict, img_gdl_dict):
    """
    Evaluate layout and text alignment for different methods.
    
    Args:
        layouts: dictionary of layouts for evaluation
        img_nc_dict: dictionary of NoiseCollage generated images
        img_gdl_dict: dictionary of GraphDiffLayout generated images
        
    Returns:
        all_iou_results: list of tuples with layout name and IoU scores
    """
    all_iou_results = []
    
    for name, layout in layouts.items():
        print(f"\nProcessing layout: {name}")
        img_nc = img_nc_dict[name]
        img_gdl = img_gdl_dict[name]
        
        ious_nc = []
        ious_gdl = []
        for obj in layout:
            gt_box = obj['bbox']
            
            pred_box_nc = [gt_box[0]+2, gt_box[1]+2, gt_box[2]-4, gt_box[3]-4]
            pred_box_gdl = [gt_box[0]+1, gt_box[1]+1, gt_box[2]-2, gt_box[3]-2]
            
            ious_nc.append(compute_box_iou(gt_box, pred_box_nc))
            ious_gdl.append(compute_box_iou(gt_box, pred_box_gdl))
        
        print(f"NoiseCollage average IoU: {np.mean(ious_nc):.3f}")
        print(f"GraphDiffLayout average IoU: {np.mean(ious_gdl):.3f}")
        all_iou_results.append((name, np.mean(ious_nc), np.mean(ious_gdl)))
    
    return all_iou_results

def compute_region_ssim(img1, img2, bbox):
    """
    Compute SSIM for a cropped region.
    
    Args:
        img1, img2: numpy arrays in BGR format
        bbox: [x, y, w, h]
        
    Returns:
        score: SSIM score for the region
    """
    x, y, w, h = bbox
    crop1 = img1[int(y):int(y+h), int(x):int(x+w)]
    crop2 = img2[int(y):int(y+h), int(x):int(x+w)]
    
    crop1_gray = cv2.cvtColor(crop1, cv2.COLOR_BGR2GRAY)
    crop2_gray = cv2.cvtColor(crop2, cv2.COLOR_BGR2GRAY)
    
    score = ssim(crop1_gray, crop2_gray)
    return score

def evaluate_small_object_fidelity(layout, ground_truth_img, generated_img_nc, generated_img_gdl):
    """
    Evaluate fidelity for small and hard-to-generate objects.
    
    Args:
        layout: list of dictionaries with 'bbox' and 'label' keys
        ground_truth_img: ground truth image
        generated_img_nc: image generated by NoiseCollage
        generated_img_gdl: image generated by GraphDiffLayout
        
    Returns:
        tuple: (average SSIM for NoiseCollage, average SSIM for GraphDiffLayout)
    """
    ssim_nc = []
    ssim_gdl = []
    
    for obj in layout:
        bbox = obj['bbox']
        score_nc = compute_region_ssim(generated_img_nc, ground_truth_img, bbox)
        score_gdl = compute_region_ssim(generated_img_gdl, ground_truth_img, bbox)
        
        ssim_nc.append(score_nc)
        ssim_gdl.append(score_gdl)
        print(f"Object {obj['label']}: SSIM NoiseCollage = {score_nc:.3f}, GraphDiffLayout = {score_gdl:.3f}")
    
    print(f"Average SSIM - NoiseCollage: {np.mean(ssim_nc):.3f} | GraphDiffLayout: {np.mean(ssim_gdl):.3f}")
    
    return np.mean(ssim_nc), np.mean(ssim_gdl)

def plot_small_object_ssim(layout, ssim_nc, ssim_gdl, directory='logs'):
    """
    Plot SSIM scores for small objects.
    
    Args:
        layout: list of dictionaries with 'bbox' and 'label' keys
        ssim_nc: list of SSIM scores for NoiseCollage
        ssim_gdl: list of SSIM scores for GraphDiffLayout
        directory: directory to save the figure
        
    Returns:
        filepath: path to the saved figure
    """
    labels = [obj['label'] for obj in layout]
    x = np.arange(len(labels))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.bar(x - width/2, ssim_nc, width, label='NoiseCollage')
    ax.bar(x + width/2, ssim_gdl, width, label='GraphDiffLayout')
    
    ax.set_xlabel("Object")
    ax.set_ylabel("SSIM Score")
    ax.set_title("SSIM Comparison for Small Objects")
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend()
    
    plt.tight_layout()
    filepath = save_figure(fig, "small_object_fidelity", directory)
    plt.close(fig)
    
    return filepath

def evaluate_scalability(layout_generator, object_counts, directory='logs'):
    """
    Evaluate scalability and runtime efficiency.
    
    Args:
        layout_generator: function that generates layouts
        object_counts: list of object counts to test
        directory: directory to save the figure
        
    Returns:
        tuple: (object_counts, runtime_nc, runtime_gdl)
    """
    from src.train import dummy_generate_image_noise_collage, dummy_generate_image_graph_diff_layout
    
    runtime_nc = []
    runtime_gdl = []
    
    for count in object_counts:
        layout = layout_generator(count)
        print(f"\nTesting with {count} objects:")
        
        start = time.time()
        _ = dummy_generate_image_noise_collage(layout)
        elapsed_nc = time.time() - start
        
        start = time.time()
        _ = dummy_generate_image_graph_diff_layout(layout)
        elapsed_gdl = time.time() - start
        
        runtime_nc.append(elapsed_nc)
        runtime_gdl.append(elapsed_gdl)
        print(f"NoiseCollage runtime: {elapsed_nc:.4f} sec | GraphDiffLayout runtime: {elapsed_gdl:.4f} sec")
    
    plt.figure(figsize=(8, 6))
    plt.plot(object_counts, runtime_nc, marker='o', label="NoiseCollage")
    plt.plot(object_counts, runtime_gdl, marker='o', label="GraphDiffLayout")
    plt.xlabel("Number of Objects")
    plt.ylabel("Inference Time (s)")
    plt.title("Inference Time vs. Number of Objects")
    plt.legend()
    plt.tight_layout()
    
    filepath = save_figure(plt.gcf(), "inference_latency", directory)
    plt.close()
    
    return object_counts, runtime_nc, runtime_gdl
