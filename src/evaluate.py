"""
Evaluation module for FahDiff experiments.
"""
import numpy as np
import networkx as nx
import torch
import scipy.stats as stats
from preprocess import set_seed


def generate_graph_from_model(model, n_nodes, device="cuda"):
    """
    Generate a graph using the trained FahDiff model.
    
    Args:
        model: Trained FahDiff model
        n_nodes: Number of nodes in the generated graph
        device: Device to run the model on
        
    Returns:
        NetworkX graph generated by the model
    """
    model.eval()
    
    # Sample from latent space
    z = torch.randn(1, model.latent_dim, device=device)
    
    # Apply force guidance if available
    if model.use_adaptive_force and hasattr(model, 'force_module') and model.force_module is not None:
        force = model.force_module(z)
        z = z + force * 0.5  # Apply moderate force guidance
    
    # Generate adjacency matrix
    with torch.no_grad():
        adj_logits = model.decode(z)
        # Convert logits to probabilities
        adj_probs = torch.sigmoid(adj_logits)
        # Binarize
        threshold = 0.5
        adj_matrix = (adj_probs > threshold).float().cpu().numpy().squeeze()
    
    # Ensure adjacency matrix is 2D
    if adj_matrix.ndim == 1:
        # For a 1D array of length n, we need to reshape to (sqrt(n), sqrt(n))
        # If n is not a perfect square, we'll use the input_dim from the model
        if hasattr(model, 'input_dim'):
            size = model.input_dim
            # If the array size doesn't match size*size, pad with zeros
            if adj_matrix.shape[0] != size*size:
                padded = np.zeros(size*size)
                padded[:adj_matrix.shape[0]] = adj_matrix
                adj_matrix = padded.reshape(size, size)
            else:
                adj_matrix = adj_matrix.reshape(size, size)
        else:
            # Fallback: try to infer the size
            size = int(np.sqrt(adj_matrix.shape[0]))
            if size*size == adj_matrix.shape[0]:
                adj_matrix = adj_matrix.reshape(size, size)
            else:
                # If we can't reshape properly, create a small valid adjacency matrix
                n = adj_matrix.shape[0]
                adj_matrix = np.eye(n)
    
    # Create graph from adjacency matrix
    G = nx.from_numpy_array(adj_matrix)
    
    return G


def compute_graph_metrics(G):
    """
    Compute graph metrics for evaluation.
    
    Args:
        G: NetworkX graph
        
    Returns:
        Dictionary of graph metrics
    """
    # Get degree sequence
    degrees = [deg for node, deg in G.degree()]
    
    # Average clustering coefficient
    avg_clustering = nx.average_clustering(G)
    
    # Average shortest path length (if connected)
    try:
        avg_path_length = nx.average_shortest_path_length(G)
    except nx.NetworkXError:
        # For disconnected graphs
        avg_path_length = float('nan')
    
    # Modularity (using community detection)
    try:
        communities = nx.algorithms.community.greedy_modularity_communities(G)
        modularity = nx.algorithms.community.quality.modularity(G, communities)
    except:
        modularity = 0.0
    
    return {
        "degrees": degrees,
        "avg_clustering": avg_clustering,
        "avg_path_length": avg_path_length,
        "modularity": modularity
    }


def compare_degree_distributions(deg_true, deg_generated, bins=20):
    """
    Compute KL divergence between degree distributions.
    
    Args:
        deg_true: True degree sequence
        deg_generated: Generated degree sequence
        bins: Number of histogram bins
        
    Returns:
        KL divergence value
    """
    # Compute histograms
    hist_true, bin_edges = np.histogram(deg_true, bins=bins, density=True)
    hist_gen, _ = np.histogram(deg_generated, bins=bin_edges, density=True)
    
    # Add small epsilon to avoid division by zero
    eps = 1e-8
    hist_true = hist_true + eps
    hist_gen = hist_gen + eps
    
    # Normalize
    hist_true = hist_true / hist_true.sum()
    hist_gen = hist_gen / hist_gen.sum()
    
    # Compute KL divergence
    kl_div = stats.entropy(hist_true, hist_gen)
    
    return kl_div


def evaluate_model(model, target_graph, config, num_samples=1):
    """
    Evaluate a trained model by generating graphs and comparing to target.
    
    Args:
        model: Trained FahDiff model
        target_graph: Target graph to compare against
        config: Configuration dictionary
        num_samples: Number of graphs to generate for evaluation
        
    Returns:
        Dictionary of evaluation metrics
    """
    set_seed(config["seed"])
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    n_nodes = target_graph.number_of_nodes()
    target_metrics = compute_graph_metrics(target_graph)
    
    # Generate multiple graphs and average metrics
    generated_metrics = {
        "kl_div_deg": [],
        "avg_clustering": [],
        "avg_path_length": [],
        "modularity": []
    }
    
    for _ in range(num_samples):
        G_gen = generate_graph_from_model(model, n_nodes, device)
        gen_metrics = compute_graph_metrics(G_gen)
        
        # Compare degree distributions
        kl_div = compare_degree_distributions(
            target_metrics["degrees"], 
            gen_metrics["degrees"]
        )
        
        generated_metrics["kl_div_deg"].append(kl_div)
        generated_metrics["avg_clustering"].append(gen_metrics["avg_clustering"])
        generated_metrics["avg_path_length"].append(gen_metrics["avg_path_length"])
        generated_metrics["modularity"].append(gen_metrics["modularity"])
    
    # Average the metrics
    avg_metrics = {k: np.mean(v) for k, v in generated_metrics.items()}
    
    return avg_metrics
