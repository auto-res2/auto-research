
model:
  base_model: "gpt2"  # Using GPT-2 as a placeholder for BTLM-3B-8K
  hidden_size: 768
  num_heads: 12
  segment_threshold: 8000  # Token count threshold for segmentation
  n_clusters: 5  # Number of clusters for segmentation

training:
  phase1:
    context_length: 512
    learning_rate: 5e-5
    batch_size: 2
    num_epochs: 1
  phase2:
    context_length: 1024
    learning_rate: 5e-5
    batch_size: 1
    num_epochs: 1

experiment:
  dropout_samples: 3  # Number of dropout samples for consensus
  retrieval_top_k: 3  # Number of top candidates to retrieve
  test_mode: true  # Set to true for quick test runs

data:
  dataset: "wikitext"
  subset: "wikitext-103-raw-v1"
  split: "train[:1%]"  # Using a small subset for testing

output:
  save_plots: true
  pdf_format: true
  plot_dpi: 300
